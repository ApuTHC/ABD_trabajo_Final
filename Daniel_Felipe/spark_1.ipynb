{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19a0e9e-3603-41ec-a532-82d41967d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Connection refused (Connection refused)\n",
      "=== Streaming Query ===\n",
      "Identifier: [id = 66e5dd64-bea9-454c-8dbd-59991acd65dc, runId = 0e29a7f9-f253-4df3-be4d-037173b9d7ec]\n",
      "Current Committed Offsets: {}\n",
      "Current Available Offsets: {TextSocketV2[host: host.docker.internal, port: 5555]: -1}\n",
      "\n",
      "Current State: ACTIVE\n",
      "Thread State: RUNNABLE\n",
      "\n",
      "Logical Plan:\n",
      "Repartition 1, true\n",
      "+- Project [data#27.latitude AS latitude#29, data#27.longitude AS longitude#30, data#27.date AS date#31, data#27.customer_id AS customer_id#32, data#27.employee_id AS employee_id#33, data#27.quantity_products AS quantity_products#34, data#27.order_id AS order_id#35]\n",
      "   +- Project [from_json(StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(date,StringType,true), StructField(customer_id,IntegerType,true), StructField(employee_id,IntegerType,true), StructField(quantity_products,IntegerType,true), StructField(order_id,StringType,true), cast(value#25 as string), Some(Etc/UTC)) AS data#27]\n",
      "      +- StreamingDataSourceV2Relation [value#25], org.apache.spark.sql.execution.streaming.sources.TextSocketTable$$anon$1@285ab4d5, TextSocketV2[host: host.docker.internal, port: 5555]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/15 06:40:26 ERROR streaming.MicroBatchExecution: Query [id = 66e5dd64-bea9-454c-8dbd-59991acd65dc, runId = 0e29a7f9-f253-4df3-be4d-037173b9d7ec] terminated with error\n",
      "java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat java.net.Socket.connect(Socket.java:556)\n",
      "\tat java.net.Socket.<init>(Socket.java:452)\n",
      "\tat java.net.Socket.<init>(Socket.java:229)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.initialize(TextSocketMicroBatchStream.scala:71)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.TextSocketMicroBatchStream.planInputPartitions(TextSocketMicroBatchStream.scala:117)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions$lzycompute(MicroBatchScanExec.scala:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.partitions(MicroBatchScanExec.scala:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:87)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MicroBatchScanExec.supportsColumnar(MicroBatchScanExec.scala:29)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:120)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:391)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:117)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$14(MicroBatchExecution.scala:577)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:567)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"data_read_write\").getOrCreate()\n",
    "\n",
    "# Crear un esquema para los datos\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"quantity_products\", IntegerType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "try:\n",
    "\n",
    "    # Leer los datos del socket\n",
    "    raw_data = spark.readStream.format(\"socket\").option(\"host\", \"host.docker.internal\").option(\"port\", 5555).load()\n",
    "\n",
    "    # Convertir los datos a DataFrame usando el esquema\n",
    "    json_df = raw_data.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "\n",
    "    # Definir el intervalo de tiempo para acumular datos\n",
    "    interval = \"30 seconds\"  # Acumular datos durante 10 minutos\n",
    "    \n",
    "    json_df = json_df.repartition(1)\n",
    "    \n",
    "    date_now = datetime.now(pytz.timezone('America/Bogota')).strftime(\"%d%m%Y_%H%M%S\")\n",
    "\n",
    "    query = json_df.writeStream \\\n",
    "            .format(\"parquet\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"path\", f\"/datalake/raw/stagging\") \\\n",
    "            .option(\"checkpointLocation\", \"/checkpoints\") \\\n",
    "            .trigger(processingTime=interval) \\\n",
    "            .start()\n",
    "    query.awaitTermination()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "finally:\n",
    "    query.stop()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208b15d-4cb4-499c-bfc4-5202104ca8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
