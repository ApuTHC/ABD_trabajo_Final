{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96d5fa3-d95e-4cd0-a8a9-f2a14d4c4e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd93a58f-7d15-46f2-8604-1b9715ba8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/datalake/raw/stagging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:458\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    453\u001b[0m recursiveFileLookup \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursiveFileLookup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    455\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    456\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter)\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/datalake/raw/stagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d7dc10-3943-4670-9bb7-7345238e5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-----------+-----------+-----------------+--------------------+\n",
      "|latitude| longitude|               date|customer_id|employee_id|quantity_products|            order_id|\n",
      "+--------+----------+-------------------+-----------+-----------+-----------------+--------------------+\n",
      "|6.198157|-75.588906|10/06/2024 13:31:01|       1325|       9024|               31|b75c2ccc-f951-499...|\n",
      "|6.249815|-75.505385|10/06/2024 13:31:23|       1219|       9026|               41|f17ee183-3358-4c9...|\n",
      "|6.244296|-75.532076|10/06/2024 13:31:02|       1363|       9018|               26|fd377924-2728-41f...|\n",
      "|6.183511|-75.637635|10/06/2024 13:31:24|       1640|       9022|               23|6a6bb2b6-2a9a-483...|\n",
      "|6.281509|-75.503671|10/06/2024 13:31:05|       1857|       9011|               24|cf1aa61f-156a-494...|\n",
      "|6.292742|-75.589281|10/06/2024 13:31:26|       1094|       9027|               40|db334b7d-3aeb-4a7...|\n",
      "|6.274129| -75.53582|10/06/2024 13:31:06|       1874|       9039|               23|ce50914f-b256-439...|\n",
      "|6.194313|-75.505413|10/06/2024 13:31:28|       1237|       9032|               10|6c102675-75d0-4d0...|\n",
      "|6.226191|-75.688527|10/06/2024 13:31:08|       1405|       9005|               20|32901ba3-44af-44d...|\n",
      "|6.257225|-75.522976|10/06/2024 13:31:10|       1270|       9018|               22|caa43d36-f882-494...|\n",
      "|6.232085|-75.662371|10/06/2024 13:31:12|       1499|       9039|               50|ad7f2e5e-0650-454...|\n",
      "|6.198248|-75.654214|10/06/2024 13:31:14|       1823|       9018|               36|ef7ff2f7-bc72-458...|\n",
      "| 6.21908|-75.602549|10/06/2024 13:31:16|       1284|       9021|               41|61a4c3c3-4f1f-4c3...|\n",
      "|6.315855|-75.681813|10/06/2024 13:31:18|       1244|       9020|               38|6ef182ed-7613-460...|\n",
      "|6.196961|-75.550665|10/06/2024 13:31:20|       1101|       9004|               31|02b9ff8d-c977-4da...|\n",
      "|6.302031|-75.570619|10/06/2024 13:31:21|       1108|       9013|               42|8678b688-bcbc-4ee...|\n",
      "|6.226163|-75.604074|10/06/2024 13:32:01|       1489|       9026|               42|5b3a311c-a907-456...|\n",
      "|6.338893|-75.686532|10/06/2024 13:32:25|       1983|       9008|               21|2931dda5-efeb-4e6...|\n",
      "|6.241216| -75.49038|10/06/2024 13:32:03|       1263|       9030|               43|30261778-5922-49d...|\n",
      "|6.285594| -75.56488|10/06/2024 13:32:27|       1793|       9001|               37|6fb6c7b2-6a87-493...|\n",
      "+--------+----------+-------------------+-----------+-----------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ab130a-7e73-4d11-a3f1-925dfd174ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadoop fs -ls /checkpoints/offsets | awk \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mprint $NF}\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m offsets_names \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output(command, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datalake/raw/stagging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, calcular_comuna_udf(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     50\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from turfpy.measurement import boolean_point_in_polygon\n",
    "from geojson import Point, Feature\n",
    "\n",
    "import subprocess\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from pyspark.sql.functions import udf, to_timestamp, year, month, dayofmonth, hour, minute, second\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "with open('barrios.json', 'r') as f:\n",
    "    barrios = json.load(f)\n",
    "\n",
    "def get_polygon_properties(geojson_data, point_feature):\n",
    "    for feature in geojson_data['features']:\n",
    "        if boolean_point_in_polygon(point_feature, feature):\n",
    "            return feature['properties']\n",
    "    return None\n",
    "\n",
    "def calcular_comuna(lat, lng):\n",
    "    point_feature = Feature(geometry=Point((lng, lat)))\n",
    "    properties = get_polygon_properties(barrios, point_feature)\n",
    "    return properties[\"NOMBRE\"], properties[\"IDENTIFICACION\"]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"neighborhood\", StringType(), False),\n",
    "    StructField(\"commune\", StringType(), False)\n",
    "])\n",
    "\n",
    "calcular_comuna_udf = udf(calcular_comuna, schema)\n",
    "\n",
    "\n",
    "command = \"hadoop fs -ls /datalake/raw/stagging | awk '{print $NF}'\"\n",
    "file_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "\n",
    "command = \"hadoop fs -ls /checkpoints/commits | awk '{print $NF}'\"\n",
    "commits_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "\n",
    "command = \"hadoop fs -ls /checkpoints/offsets | awk '{print $NF}'\"\n",
    "offsets_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "\n",
    "df = spark.read.parquet(\"/datalake/raw/stagging\")\n",
    "\n",
    "\n",
    "df = df.withColumn(\"result\", calcular_comuna_udf(df[\"latitude\"], df[\"longitude\"]))\n",
    "df = df.select(\"*\", \"result.*\").drop(\"result\")\n",
    "\n",
    "df = df.withColumn(\"date\", to_timestamp(df[\"date\"], \"dd/MM/yyyy HH:mm:ss\"))\n",
    "df = df.withColumn(\"day\", dayofmonth(df[\"date\"]))\\\n",
    "       .withColumn(\"month\", month(df[\"date\"]))\\\n",
    "       .withColumn(\"year\", year(df[\"date\"]))\\\n",
    "       .withColumn(\"hour\", hour(df[\"date\"]))\\\n",
    "       .withColumn(\"minute\", minute(df[\"date\"]))\\\n",
    "       .withColumn(\"second\", second(df[\"date\"]))\n",
    "\n",
    "# date_now = datetime.now(pytz.timezone('America/Bogota')).strftime(\"%d%m%Y_%H%M%S\")\n",
    "# path_write = f\"/datalake/silver/stagging/{date_now}\"\n",
    "path_write = f\"/datalake/silver/stagging/\"\n",
    "\n",
    "df.write.parquet(path_write)\n",
    "\n",
    "for name in file_names:\n",
    "    if \".parquet\" in name:\n",
    "        command = f\"hadoop fs -mv {name} /datalake/raw/ingested\"\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        \n",
    "for commit in commits_names:\n",
    "    if commit != \"items\":\n",
    "        command = f\"hadoop fs -mv {commit} /checkpoints/moved\"\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        \n",
    "for offset in offsets_names:\n",
    "    if offset != \"items\":\n",
    "        command = f\"hadoop fs -mv {offset} /checkpoints/moved\"\n",
    "        subprocess.run(command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b220dc-e741-4ead-a3d9-f4bf8118cdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting turfpy\n",
      "  Downloading turfpy-0.0.7.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting geojson\n",
      "  Downloading geojson-3.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from turfpy) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from turfpy) (1.8.1)\n",
      "Collecting shapely\n",
      "  Downloading shapely-2.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m634.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: turfpy\n",
      "  Building wheel for turfpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for turfpy: filename=turfpy-0.0.7-py3-none-any.whl size=39101 sha256=0f89ab1b6c0891a246f4645575a072faf76497ec62eca39d97c868e3aa642070\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/f8/07/965ea3fe9ce3d94e9ee6815425f294500adc31eb4c14037c61\n",
      "Successfully built turfpy\n",
      "Installing collected packages: shapely, geojson, turfpy\n",
      "Successfully installed geojson-3.1.0 shapely-2.0.4 turfpy-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='24.0'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install turfpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899c8ef7-437d-4f37-91ef-7329ae511d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['items', '/datalake/raw/stagging/10062024_141812', '']\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-054d0c3b-4a08-4ae5-881b-3b4487d5d5ef-c000.snappy.parquet\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-1c320fb2-5168-4839-8b23-f75d57e7db11-c000.snappy.parquet\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-402f848e-be34-4202-8748-d3ef8e92a55f-c000.snappy.parquet\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-70c9fd08-1f5c-440d-8544-a540877cc622-c000.snappy.parquet\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-e6c5b953-1817-4dca-93c1-a85dd97d163c-c000.snappy.parquet\n",
      "/datalake/raw/stagging/10062024_141812/part-00000-e9fd3204-ad1c-43f8-8843-3382876ee586-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"hadoop fs -ls /datalake/raw/stagging | awk '{print $NF}'\"\n",
    "dir_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "print(dir_names)\n",
    "for dir_name in dir_names:\n",
    "    if dir_name != \"items\":\n",
    "        command = f\"hadoop fs -ls {dir_name}\" + \"| awk '{print $NF}'\"\n",
    "        file_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "        for name in file_names:\n",
    "            if \".parquet\" in name:\n",
    "                print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1095b3-0d9a-498c-95a3-feb0f7ac5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['items', '/checkpoints/commits/0', '/checkpoints/commits/1', '/checkpoints/commits/2', '/checkpoints/commits/3', '/checkpoints/commits/4', '/checkpoints/commits/5', '']\n"
     ]
    }
   ],
   "source": [
    "command = \"hadoop fs -mv /datalake/raw/stagging/pruebis /datalake/raw/ingested\"\n",
    "subprocess.run(command, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d471d6e-a5f7-4762-b832-0953737f963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/datalake/raw/ingested\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0029e1e-da3d-4321-9a96-915de1cb3c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
