{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(event) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Esperar 30 segundos antes de generar el siguiente evento\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Definir las coordenadas aproximadas de Medellín\n",
    "lat_range = (6.217, 6.317)  # Latitud de Medellín\n",
    "lon_range = (-75.567, -75.467)  # Longitud de Medellín\n",
    "\n",
    "# Lista de comunas y barrios de Medellín\n",
    "comunas = [\"COMUNA 1\", \"COMUNA 2\", \"COMUNA 3\", \"COMUNA 4\", \"COMUNA 5\", \"COMUNA 6\", \"COMUNA 7\", \"COMUNA 8\", \"COMUNA 9\", \"COMUNA 10\", \"COMUNA 11\", \"COMUNA 12\", \"COMUNA 13\", \"COMUNA 14\", \"COMUNA 15\", \"COMUNA 16\"]\n",
    "barrios = [\"BARRIO 1\", \"BARRIO 2\", \"BARRIO 3\", \"BARRIO 4\", \"BARRIO 5\", \"BARRIO 6\", \"BARRIO 7\", \"BARRIO 8\", \"BARRIO 9\", \"BARRIO 10\"]\n",
    "\n",
    "# Generar datos simulados\n",
    "def generate_event():\n",
    "    event = {\n",
    "        \"latitude\": round(random.uniform(*lat_range), 6),\n",
    "        \"longitude\": round(random.uniform(*lon_range), 6),\n",
    "        \"date\": datetime.now(pytz.timezone('America/Bogota')).strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "        \"customer_id\": random.randint(1000, 2000),\n",
    "        \"employee_id\": random.randint(9000, 10000),\n",
    "        \"quantity_products\": random.randint(1, 50),\n",
    "        \"order_id\": str(uuid.uuid4()),\n",
    "        \"commune\": random.choice(comunas),\n",
    "        \"neighborhood\": random.choice(barrios),\n",
    "        \"partition_date\": datetime.now().strftime(\"%d%m%Y\"),\n",
    "        \"event_date\": datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "        \"event_day\": datetime.now().day,\n",
    "        \"event_hour\": datetime.now().hour,\n",
    "        \"event_minute\": datetime.now().minute,\n",
    "        \"event_month\": datetime.now().month,\n",
    "        \"event_second\": datetime.now().second,\n",
    "        \"event_year\": datetime.now().year\n",
    "    }\n",
    "    return event\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        event = generate_event()\n",
    "        # Aquí se puede guardar el evento en un archivo JSON, base de datos, etc.\n",
    "        # Ejemplo de guardado en un archivo JSON\n",
    "        with open('simulated_data.json', 'a') as file:\n",
    "            file.write(json.dumps(event) + \"\\n\")\n",
    "        # Esperar 30 segundos antes de generar el siguiente evento\n",
    "        time.sleep(30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Ruta al archivo Parquet\u001b[39;00m\n\u001b[0;32m      4\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../base.data/50001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    135\u001b[0m     concat,\n\u001b[0;32m    136\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     qcut,\n\u001b[0;32m    149\u001b[0m )\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     extensions,\n\u001b[0;32m      4\u001b[0m     indexers,\n\u001b[0;32m      5\u001b[0m     interchange,\n\u001b[0;32m      6\u001b[0m     types,\n\u001b[0;32m      7\u001b[0m     typing,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Expanding,\n\u001b[0;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     Window,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     read_json,\n\u001b[0;32m      3\u001b[0m     to_json,\n\u001b[0;32m      4\u001b[0m     ujson_dumps,\n\u001b[0;32m      5\u001b[0m     ujson_loads,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     build_table_schema,\n\u001b[0;32m     69\u001b[0m     parse_table_schema,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m         Hashable,\n\u001b[0;32m     76\u001b[0m         Mapping,\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     TextFileReader,\n\u001b[0;32m      3\u001b[0m     TextParser,\n\u001b[0;32m      4\u001b[0m     read_csv,\n\u001b[0;32m      5\u001b[0m     read_fwf,\n\u001b[0;32m      6\u001b[0m     read_table,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Big data-curso\\Trabajo Final\\ABD_trabajo_Final\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     AbstractMethodError,\n\u001b[0;32m     35\u001b[0m     ParserWarning,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "File \u001b[1;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "def send_data():\n",
    "    host = 'localhost'  # Nombre del host o dirección IP\n",
    "    port = 9999  # Puerto en el que escucha el servidor\n",
    "\n",
    "    s = socket.socket()\n",
    "    s.bind((host, port))\n",
    "    s.listen(1)\n",
    "    print(f\"Listening on {host}:{port}\")\n",
    "\n",
    "    conn, addr = s.accept()\n",
    "    print(f\"Connection from {addr}\")\n",
    "\n",
    "    with open('simulated_data.json', 'r') as file:\n",
    "        for line in file:\n",
    "            conn.send(line.encode())\n",
    "            time.sleep(1)  # Ajustar la frecuencia de envío según sea necesario\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    send_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"UNALWaterStream\").getOrCreate()\n",
    "\n",
    "# Crear un esquema para los datos\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"quantity_products\", IntegerType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"commune\", StringType(), True),\n",
    "    StructField(\"neighborhood\", StringType(), True),\n",
    "    StructField(\"partition_date\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_day\", IntegerType(), True),\n",
    "    StructField(\"event_hour\", IntegerType(), True),\n",
    "    StructField(\"event_minute\", IntegerType(), True),\n",
    "    StructField(\"event_month\", IntegerType(), True),\n",
    "    StructField(\"event_second\", IntegerType(), True),\n",
    "    StructField(\"event_year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Leer los datos del socket\n",
    "raw_data = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Convertir los datos a DataFrame usando el esquema\n",
    "json_df = raw_data.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Mostrar los datos en consola (para pruebas)\n",
    "query = json_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Esperar a que el streaming finalice\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensayo hive\n",
    "\n",
    "Para almacenar los datos procesados por Spark Streaming en una base de datos distribuida de Hive, puedes seguir los siguientes pasos:\n",
    "\n",
    "Configurar Hive en Spark:\n",
    "\n",
    "Asegúrate de tener un clúster de Hive configurado y accesible.\n",
    "Configura Spark para interactuar con Hive.\n",
    "Crear una Tabla en Hive:\n",
    "\n",
    "Define la estructura de la tabla en Hive donde deseas almacenar los datos.\n",
    "Guardar Datos en la Tabla de Hive desde Spark:\n",
    "\n",
    "Una vez que los datos estén en el DataFrame de Spark, escribe estos datos en la tabla de Hive.\n",
    "Paso 1: Configurar Hive en Spark\n",
    "Asegúrate de que Spark esté configurado para usar Hive. Esto generalmente implica tener las bibliotecas de Hive en el classpath de Spark y configuraciones adecuadas en spark-defaults.conf y hive-site.xml.\n",
    "\n",
    "Paso 2: Crear una Tabla en Hive\n",
    "Puedes crear una tabla en Hive usando la CLI de Hive o a través de un script HiveQL. A continuación se muestra un ejemplo de cómo crear una tabla en Hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE water_sales (\n",
    "    latitude DOUBLE,\n",
    "    longitude DOUBLE,\n",
    "    date STRING,\n",
    "    customer_id INT,\n",
    "    employee_id INT,\n",
    "    quantity_products INT,\n",
    "    order_id STRING,\n",
    "    commune STRING,\n",
    "    neighborhood STRING,\n",
    "    partition_date STRING,\n",
    "    event_date STRING,\n",
    "    event_day INT,\n",
    "    event_hour INT,\n",
    "    event_minute INT,\n",
    "    event_month INT,\n",
    "    event_second INT,\n",
    "    event_year INT\n",
    ") STORED AS PARQUET;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Crear la sesión de Spark con soporte para Hive\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UNALWaterStream\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crear un esquema para los datos\n",
    "schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"quantity_products\", IntegerType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"commune\", StringType(), True),\n",
    "    StructField(\"neighborhood\", StringType(), True),\n",
    "    StructField(\"partition_date\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_day\", IntegerType(), True),\n",
    "    StructField(\"event_hour\", IntegerType(), True),\n",
    "    StructField(\"event_minute\", IntegerType(), True),\n",
    "    StructField(\"event_month\", IntegerType(), True),\n",
    "    StructField(\"event_second\", IntegerType(), True),\n",
    "    StructField(\"event_year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Leer los datos del socket\n",
    "raw_data = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Convertir los datos a DataFrame usando el esquema\n",
    "json_df = raw_data.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Definir una función para guardar los datos en Hive\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.write.mode(\"append\").saveAsTable(\"water_sales\")\n",
    "\n",
    "# Configurar la consulta de streaming para escribir en Hive\n",
    "query = json_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .start()\n",
    "\n",
    "# Esperar a que el streaming finalice\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación\n",
    "Configuración de Spark con Soporte para Hive:\n",
    "\n",
    "enableHiveSupport() se usa para habilitar el soporte de Hive en la sesión de Spark.\n",
    "Esquema de Datos:\n",
    "\n",
    "Define el esquema de los datos que se recibirán y procesarán.\n",
    "Lectura de Datos desde el Socket:\n",
    "\n",
    "raw_data lee los datos del socket.\n",
    "Conversión a DataFrame:\n",
    "\n",
    "json_df convierte los datos a un DataFrame estructurado según el esquema definido.\n",
    "Función foreach_batch_function:\n",
    "\n",
    "Esta función se llama para cada micro-lote de datos y guarda los datos en la tabla de Hive usando saveAsTable.\n",
    "Configuración de la Consulta de Streaming:\n",
    "\n",
    "La consulta de streaming se configura para usar foreachBatch para guardar los datos en la tabla de Hive.\n",
    "Con este enfoque, estarás almacenando los datos de tu flujo de streaming directamente en una tabla de Hive, permitiendo así su posterior análisis y procesamiento en un entorno de datos distribuido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
