{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['items', '/datalake/silver/stagging/17062024_190437', '']\n",
      "['/datalake/silver/stagging/17062024_190437']\n",
      "+--------------------+-------------------+-----------+-------------------+-------------+---------+----------+------------+-----------+------------+----------+--------+----------+----------------+--------------------+-----------------+-------------+\n",
      "|             commune|current_temperature|customer_id|               date|  employee_id|event_day|event_hour|event_minute|event_month|event_second|event_year|latitude| longitude|    neighborhood|            order_id|quantity_products|sale_point_id|\n",
      "+--------------------+-------------------+-----------+-------------------+-------------+---------+----------+------------+-----------+------------+----------+--------+----------+----------------+--------------------+-----------------+-------------+\n",
      "|Comuna 8 - Villa ...|               22.9|       1016|15/06/2024 16:35:11|Employee_0_wd|       15|        16|          35|          6|          11|      2024|6.247303|-75.552002|          Enciso|b48dfb6a-cc61-40f...|               34|            0|\n",
      "|Comuna 11 - Laure...|               22.9|       1592|15/06/2024 16:35:12|Employee_1_wd|       15|        16|          35|          6|          12|      2024|6.254075|-75.579864|    Suramericana|b21acef6-b24e-414...|               22|            1|\n",
      "|  Comuna 7 - Robledo|               22.9|       1246|15/06/2024 16:35:13|Employee_2_wd|       15|        16|          35|          6|          13|      2024|6.272651|-75.579955|Cerro El Volador|dc369e81-942a-455...|               31|            2|\n",
      "|Comuna 8 - Villa ...|               22.9|       1218|15/06/2024 16:35:14|Employee_3_wd|       15|        16|          35|          6|          14|      2024|6.246319|-75.552374|           Sucre|26ac4d09-7ee3-4a5...|               23|            3|\n",
      "|Comuna 10 - La Ca...|               22.9|       1202|15/06/2024 16:35:15|Employee_4_wd|       15|        16|          35|          6|          15|      2024|6.241956|-75.566376|      Las Palmas|43bd60dc-f805-4ed...|               41|            4|\n",
      "+--------------------+-------------------+-----------+-------------------+-------------+---------+----------+------------+-----------+------------+----------+--------+----------+----------------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n",
    "\n",
    "command = \"hadoop fs -ls /datalake/silver/stagging | awk '{print $NF}'\"\n",
    "dir_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "print(dir_names)\n",
    "array_silver =[]\n",
    "for dir_name in dir_names:\n",
    "    if (dir_name != \"items\" and dir_name != \"\") :\n",
    "        array_silver.append(dir_name)\n",
    "        command = f\"hadoop fs -ls {dir_name}\" + \"| awk '{print $NF}'\"\n",
    "        file_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "        for name in file_names:\n",
    "            if \".parquet\" in name:\n",
    "                #print(name)\n",
    "                pass\n",
    "                \n",
    "print(array_silver)\n",
    "df = spark.read.parquet(*array_silver)\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql.functions import sum as _sum, max as _max, concat_ws, col, udf, to_timestamp, year, month, dayofmonth, hour, minute, second, to_date, date_format, row_number, desc\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+-----------+\n",
      "|event_day|event_month|   employee_id|total_sales|\n",
      "+---------+-----------+--------------+-----------+\n",
      "|       15|          6|Employee_69_wd|        296|\n",
      "|       15|          6|Employee_73_wd|        284|\n",
      "|       15|          6|Employee_18_wd|        252|\n",
      "|       15|          6| Employee_5_wd|        236|\n",
      "|       15|          6|Employee_80_wd|        220|\n",
      "|       15|          6|Employee_78_wd|        215|\n",
      "|       15|          6|Employee_59_wd|        212|\n",
      "|       15|          6|Employee_44_wd|        208|\n",
      "|       15|          6|Employee_99_wd|        205|\n",
      "|       16|          6|Employee_63_wd|        203|\n",
      "|       15|          6|Employee_11_wd|        200|\n",
      "|       15|          6| Employee_9_wd|        192|\n",
      "|       15|          6|Employee_15_wd|        188|\n",
      "|       15|          6| Employee_0_wd|        188|\n",
      "|       15|          6|Employee_71_wd|        188|\n",
      "|       15|          6|Employee_91_wd|        183|\n",
      "|       15|          6|Employee_63_wd|        183|\n",
      "|       15|          6|Employee_30_wd|        183|\n",
      "|       15|          6|Employee_46_wd|        181|\n",
      "|       15|          6|Employee_52_wd|        179|\n",
      "+---------+-----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_seller_aggregated = df.groupBy(\"event_day\",\"event_month\", \"employee_id\") \\\n",
    "    .agg(_sum(\"quantity_products\").alias(\"total_sales\")).orderBy(col(\"total_sales\").desc())\n",
    "df_seller_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|sale_point_id|total_store|\n",
      "+-------------+-----------+\n",
      "|           63|        400|\n",
      "|           73|        375|\n",
      "|           69|        369|\n",
      "|           78|        361|\n",
      "|            5|        343|\n",
      "|           80|        318|\n",
      "|           27|        310|\n",
      "|           50|        304|\n",
      "|           43|        303|\n",
      "|           71|        286|\n",
      "|           10|        285|\n",
      "|           23|        282|\n",
      "|           47|        279|\n",
      "|           91|        274|\n",
      "|           30|        265|\n",
      "|           72|        265|\n",
      "|           15|        264|\n",
      "|           24|        257|\n",
      "|           52|        254|\n",
      "|           18|        252|\n",
      "+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar los datos por fecha y tienda para calcular el total de ventas en esa tienda\n",
    "df_store_aggregated = df.groupBy(\"sale_point_id\") \\\n",
    "    .agg(_sum(\"quantity_products\").alias(\"total_store\")).orderBy(col(\"total_store\").desc())\n",
    "df_store_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             commune|total_commune|\n",
      "+--------------------+-------------+\n",
      "|Comuna 14 - El Po...|         2413|\n",
      "| Comuna 5 - Castilla|         2217|\n",
      "|  Comuna 7 - Robledo|         1878|\n",
      "|Comuna 8 - Villa ...|         1834|\n",
      "| Comuna 3 - Manrique|         1491|\n",
      "|   Comuna 16 - Bel√©n|         1360|\n",
      "|Comuna 9 - Buenos...|         1179|\n",
      "| Comuna 4 - Aranjuez|         1116|\n",
      "|Comuna 15 - Guayabal|          985|\n",
      "|Comuna 11 - Laure...|          965|\n",
      "|Comuna 10 - La Ca...|          947|\n",
      "|  Comuna 1 - Popular|          758|\n",
      "|Comuna 12 - La Am...|          599|\n",
      "|Comuna 6 - Doce d...|          474|\n",
      "|Comuna 13 - San J...|          244|\n",
      "|Comuna 2 - Santa ...|          228|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar los datos por fecha y tienda para calcular el total de ventas en esa tienda\n",
    "df_store_aggregated = df.groupBy(\"commune\") \\\n",
    "    .agg(_sum(\"quantity_products\").alias(\"total_commune\")).orderBy(col(\"total_commune\").desc())\n",
    "df_store_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+\n",
      "|event_day|event_month|max(total_sales)|\n",
      "+---------+-----------+----------------+\n",
      "|       15|          6|             296|\n",
      "|       17|          6|              50|\n",
      "|       16|          6|             203|\n",
      "+---------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar para obtener solo el vendedor con m√°s ventas cada d√≠a\n",
    "df_top_seller = df_seller_aggregated.groupBy(\"event_day\",\"event_month\") \\\n",
    "                .max(\"total_sales\")\n",
    "df_top_seller.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
