{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ab130a-7e73-4d11-a3f1-925dfd174ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copiado:/datalake/raw/stagging/part-00000-0b1e19c2-7021-4320-b630-c32d50d2a548-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-2719f546-4cf1-436d-97f7-fa5f3706fa68-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-362ae5c9-ada2-4a03-bb93-3931cea988ce-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-3983f2f5-717b-49f3-8f0a-4951b03503d8-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-4dbe635f-098e-494e-88ec-6027c9a38798-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-5bb35691-ab00-4943-b080-b5d4be7b7d63-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-87cec15e-eda3-4b80-9cab-1ab32ec76c53-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-92fa1d8e-7f4e-4fdd-bf5c-2426611709d9-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-97e8caeb-2935-4c46-b290-3fc13bb6ff47-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-a079f9ed-5d32-45cc-bc8b-68c4cc9ba8ff-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-daf610fa-ee5f-4c31-9f61-d5486438ba70-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-dd3738b7-000c-473f-8c09-0d7021b92172-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-f2e5da73-84cb-47f8-9b07-1e2bd697b31c-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-6db26f43-bb22-4e01-a831-4a360365ccb0-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-e99e4a54-7317-4da8-a7d1-f7978f42617a-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-3ddf30ca-49e3-4489-b166-387b955a4b1e-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-472bf529-64e8-4ff6-9d07-e2fe141af08d-c000.snappy.parquet\n",
      "copiado:/datalake/raw/stagging/part-00000-c9d986e5-34bb-4ec3-bb6c-b4c65f1a5366-c000.snappy.parquet\n",
      "Error: Unable to infer schema for Parquet. It must be specified manually.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from pyspark.sql.functions import udf, to_timestamp, year, month, dayofmonth, hour, minute, second\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n",
    "\n",
    "names_history = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        command = \"hadoop fs -ls /datalake/raw/stagging | awk '{print $NF}'\"\n",
    "        file_names = subprocess.check_output(command, shell=True).decode().split('\\n')\n",
    "\n",
    "        array_names = []\n",
    "\n",
    "        for name in file_names:\n",
    "            if \".parquet\" in name:\n",
    "                if name not in names_history:\n",
    "                    array_names.append(name)\n",
    "                    names_history.append(name)\n",
    "\n",
    "        df = spark.read.parquet(*array_names)\n",
    "\n",
    "\n",
    "        date_now = datetime.now(pytz.timezone('America/Bogota')).strftime(\"%d%m%Y_%H%M%S\")\n",
    "        path_write = f\"/datalake/silver/stagging/{date_now}\"\n",
    "\n",
    "\n",
    "        df.write.parquet(path_write)\n",
    "\n",
    "        for name in array_names:\n",
    "            if \".parquet\" in name:\n",
    "                name_new = name.replace(\"stagging\", \"ingested\")\n",
    "                command = f\"hadoop fs -cp {name} {name_new}\"\n",
    "                subprocess.run(command, shell=True, check=True)\n",
    "                print(f\"copiado:{name}\")\n",
    "                \n",
    "        time.sleep(30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Deteniendo lectura...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd9a5-ce56-417b-9f68-b218e8c33986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
